{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.1 Hidden Markov Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We view the world as a series of frames or snapshots in time, each of which contains a set of random variables where some are observable, and some are not. Hidden Markov Models or HMM's, are a machine learning technique used to make sense of sequences in time. HMM's have recently come to dominate the field of speech recognition. These models are good at taking frames that make up an input signal and feedback a result based on that model. HMM's are based on a rigourous set of mathematical theory which allowed speech researchers to build onto previous mathematical results developed in other fields. HMM's must be trained for a particular end result. In speech recognition, this could be a word or  series of words. In HMMS' these models are able to distinguish a signal with noise present, or different magnitudes, or even durations of states. This is the power available in HMM's.\n",
    "\n",
    "This lesson will cover the algorithm and methods for HMM's and their applications for use in detecting patterns in vibrational signitures in aeromechanical testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models are state based models of data used to recognize patterns in the data. Given a set of states S = {s1, s2, ..., si}, the series can be observed over time. For visualization, lets look at an example. Consider the following graph representing 4 states:\n",
    "\n",
    "![graph](./images/graph.PNG)\n",
    "\n",
    "Given these 4 states, we can compute the transition probabilities as follows:\n",
    "\n",
    "1. State 1 consumes 10 frames. Therefore, the outpout probability is 0.1 and the probability of remaining in step 1 is 0.9.\n",
    "2. State 2 consumes 5 frames. Therefore, its transition probabilities are 0.2 and 0.8.\n",
    "3. State 3 consumes 20 frames. Therefore, its transistion probabilities are 0.05 and 0.95\n",
    "4. State 4 consumes 3 frames. Therefore, its transition probabilites are 0.333 and 0.667\n",
    "\n",
    "After calculating our transisition probabilities, we can draw the following Markov Model:\n",
    "\n",
    "![examplehmm](./images/examplehmm.PNG)\n",
    "\n",
    "Next, we need to consider the start probabilities. There are 2 different ways to look at this. First, the model could be as shown, and the entrance point to the model is only at state 1. However, we can have probable outcomes where the starting point is in the middle of the model. We can represent this in the following way, where the entrance probabilities are uniform.\n",
    "\n",
    "![examplehmm2](./images/examplehmm2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Forward-Backward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "HMM's are a temporal probabalistic models in which the state of the process is described by a single descrete variable. The possible values of the variable are the possible states of the world. We can represent HMM's in the following way:\n",
    "\n",
    "$X_{t}$ = A single descrete state variable  \n",
    "$S$ = Number of possible states\n",
    "\n",
    "The transition model is represented by $P(X_{t} | X_{t-1})$ as an SxS matrix T where $T_{ij} = P(X_{t}=j | X_{t-1}=i)$ where $T_{ij}$ = the probability of transition from state i to state j.\n",
    "\n",
    "Next, we look at the evidence and observations:\n",
    "\n",
    "$e_{t}$ = Evidence variable at time, t\n",
    "\n",
    "$P(e_{t} | X_{t})$ for each state, i for how likely it is that state i causes evidence $e_{t}$ to appear. These probabilities are represented by an SxS diagonal matrix, $O_{t}$.\n",
    "\n",
    "Given the above, we can compute the forward equation by simpel matrix-vector operations:\n",
    "\n",
    "$$f_{1:t+1} = \\alpha O_{t+1}T^{T}f_{1:t}$$\n",
    "\n",
    "and the backward equation:\n",
    "\n",
    "$$b_{k+1:t} = TO_{k+1}b_{k+2:t}$$\n",
    "\n",
    "These equations represent the forward-backward algorithm applied to a sequence of lenght *t*. The forward-backward algorithm is an inference algorithm for HMM's which compute the posterior marginals of all hidden state variables given a sequence of observations and emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Viterbi Trellis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vrterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, called the Viterbi path, that results in a sequence of observed events. The Viterbi algorithm can be built and represented as a trellis, where all possible paths are determined, and take into account the transition and emission probabilities of each state to determine the best path. Let's take our HMM model displayed previously and represent that as a Viterbi Trellis:\n",
    "\n",
    "![VT](./images/viterbiTrellis.PNG)\n",
    "\n",
    "Notice the filling in of the transition probabilities and that each state is a row on the trellis. The trellis displayed is a simple linear model where we must enter state 1 before 2 and state 2 before state 3, etc. However, a trellis can be built for models that have several combinational types such as state 1 to state 3, skipping state 2.\n",
    "\n",
    "Next, we take an observation made such as the sequence: {-2,-1,-1,0,1,0,1,2,2} and compute the probability of each value being generated by a state on the trellis. For this example we will use the following:\n",
    "\n",
    "$P(S_{1} | X_{t} = -2) = 1.0$  \n",
    "$P(S_{1} | X_{t} = -1) = 0.5$  \n",
    "$P(S_{2} | X_{t} = -1) = 0.5$  \n",
    "$P(S_{2} | X_{t} = 0) = 0.5$  \n",
    "$P(S_{3} | X_{t} = 0) = 0.5$  \n",
    "$P(S_{3} | X_{t} = 1) = 0.5$  \n",
    "$P(S_{4} | X_{t} = 1) = 0.5$  \n",
    "$P(S_{4} | X_{t} = 2) = 1.0$  \n",
    "\n",
    "This yields the following result with the probabilities placed in the circle of the trellis:\n",
    "\n",
    "![VT2](./images/viterbiTrellis2.PNG)\n",
    "\n",
    "Next, we compute the path probabilities and choose the path with the highest probability. We compute these by multiplying each state value probability by the previous transition probability, and multiplying each subsequent step. By doing this, we get the following where each step probability is shown in red and the path in green:\n",
    "\n",
    "![VT3](./images/viterbiTrellis3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Example: Rainy or sunny?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this example, suppose there is a person who, for whatever reason, is unable to observe the current state of the weather. Instead, they must make inferences based on discussions made with a friend. Here, we have 2 possible hidden states as \"Sunny\" or \"Rainy\". Given a set of probabilities for the Markov Model, the person must make inferences based on the actions of their friend. These actions are \"Walking\", \"Cleaning\", and \"Shopping\". Please view the example code below, provide observations, prior probabilities, emission probabilities, and transition probabilities and observe how the system responds.\n",
    "\n",
    "An example Markov Model can be seen below:\n",
    "\n",
    "![exampleChain](./images/exampleChain.PNG)\n",
    "***Note:*** *Example adapted from Wikipedia resource detailed in resources which expands on the example given in the AIMA book*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57159c7b58f64e579ec1c53c915859d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from Example1_Widgets import *\n",
    "\n",
    "# The Viterbi Algorithm\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = max(V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] for prev_st in states)\n",
    "            for prev_st in states:\n",
    "                if V[t-1][prev_st][\"prob\"] * trans_p[prev_st][st] == max_tr_prob:\n",
    "                    max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "#    for line in dptable(V):\n",
    "#        print line\n",
    "    opt = []\n",
    "    # The highest probability\n",
    "    max_prob = max(value[\"prob\"] for value in V[-1].values())\n",
    "    previous = None\n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] == max_prob:\n",
    "            opt.append(st)\n",
    "            previous = st\n",
    "            break\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "    return (opt,max_prob)\n",
    "        \n",
    "def RunExample1(b):\n",
    "    states = ('Rainy', 'Sunny')\n",
    "    observations = str(Observations.value).split(',')\n",
    "    start_probability = {'Rainy': R_Start.value, 'Sunny': 1-R_Start.value}\n",
    "    transition_probability = {\n",
    "       'Rainy' : {'Rainy': R_Transition.value, 'Sunny': 1-R_Transition.value},\n",
    "       'Sunny' : {'Rainy': S_Transition.value, 'Sunny': 1-S_Transition.value},\n",
    "       }\n",
    "    emission_probability = {\n",
    "       'Rainy' : {'walk': R_Walk.value, 'shop': R_Shop.value, 'clean': R_Clean.value},\n",
    "       'Sunny' : {'walk': S_Walk.value, 'shop': S_Shop.value, 'clean': S_Clean.value},\n",
    "       }\n",
    "    opt,prob = viterbi(observations,states,start_probability,transition_probability,emission_probability)\n",
    "    solutionWidget.value = str(opt)\n",
    "    \n",
    "#Display Widgets\n",
    "runButton.on_click(RunExample1)\n",
    "widgets.VBox([bx1,bx2,bx3,bx4,bx5,bx6,bx7,bx8,bx9,bx10,runButton,bx11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aeromechanical Example: Recognize Engine Order Vibrational Signiture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In aeromechanical testing, we can apply a similar process as speech recognition to build a system that can detect patterns in acquisition data. In particular, we will look at building an agent to recognize when an integral vibratory response occurs. This is done (as in speech recognition) by first placing the data in the Frequency Vs. Time domain with magnitude as the 3rd axis. This is also known as a Spectrogram or maybe a time based campbell zmod. An example of this plot can be seen below:\n",
    "\n",
    "![zmod](./images/ZMod.png)\n",
    "\n",
    "After we acquire or simulate several data sets, we can determine the number of frames needed and our transition probabilities. We do this by use of the EM algorithm with Gaussian Mixture Models as presented in the previous section. First, we will run EM several times on the data sets and increase or decrease our cluster count each time, We will take these and run the result clusters throught a Bayesion Info Criterion function that will determine the best cluster count to use. The cluster count will be the number of frames we need to split the data into based on our clusters. An example of this can be seen below as a 5 frame model:\n",
    "\n",
    "![zmod2](./images/ZMod2.png)\n",
    "\n",
    "Next, we determine our probabilities for our priors, transitions, and emissions. We do this based on time. We compute the probability of the data being in state i at time t. We can determine our probabilities as follows:\n",
    "\n",
    "![zmod2](./images/ZMod3.png)\n",
    "\n",
    ">  \n",
    "This yields the transition probabilities of:\n",
    ">  \n",
    "1. State 1 (Normal State) = 0.11, 0.89\n",
    "2. State 2 (EO Entrant State) = 0.125, 0.875\n",
    "3. State 3 (EO Response State) = 0.11, 0.89\n",
    "4. State 4 (EO Exit State) = 0.125, 0.875\n",
    "5. State 5 (Normal State) = 0.125, 0.875\n",
    "\n",
    "Finally, we build our model.\n",
    "\n",
    "![eohmm](./images/eoHmm.PNG)\n",
    "\n",
    "Now, when we run data through this model, the data values will be assigned to states based on their gaussian distribution. The probability of the data belonging to a state as well as the transition probabilities will factor into how the model is traversed. Also, depending on how the model was trained, the model will be able to support a wide range of variations such as length of time in a state, magnitudes, and frequency.\n",
    "\n",
    "***Note:*** *Due to the complexity of this example, it was not feasable to create an example program. The Running of the algorithm is similar to example 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
